# -*- coding: utf-8 -*-
"""SRGAN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ylUhtMiWu5GUd6kjRmwB-2uyHVEys6az
"""

# from google.colab import files
# files.upload()

# !mkdir /root/.kaggle
# !mv kaggle.json /root/.kaggle/
# !kaggle datasets download -d jessicali9530/celeba-dataset
# !unzip -qq /content/celeba-dataset.zip

# !mkdir saved_models

import numpy as np
import pandas as pd

import os
import random
from tqdm import tqdm
import matplotlib.pyplot as plt

from PIL import Image

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import torchvision
from torchvision import transforms
import torch.nn.functional as F
from torch.autograd import Variable
from torchvision.models import vgg19
from torchvision.utils import save_image, make_grid

from sklearn.model_selection import train_test_split

# Settings for Training

device = "cuda" if torch.cuda.is_available() else "cpu"

hrHeight, hrWidth = 256, 256
hrShape = (hrHeight, hrWidth)
downSampleRatio = 4

batchSize = 4

channels = 3

lr = 0.0008
b1 = 0.5
b2 = 0.999

class celebADataset(Dataset):
    def __init__(self, path, transform = None):
        self.path = path
        self.files = os.listdir(path)
        
        self.hrTransform = transforms.Compose([
            transforms.Resize((hrHeight, hrWidth), interpolation = torchvision.transforms.InterpolationMode.BICUBIC),
            transforms.ToTensor()
        ])
        
        self.lrTransform = transforms.Compose([
            transforms.Resize((hrHeight // downSampleRatio, hrWidth // downSampleRatio), interpolation = torchvision.transforms.InterpolationMode.BICUBIC),
            transforms.ToTensor()
        ])
        
    def __len__(self):
        return len(self.files)
    
    def __getitem__(self, idx):
        pathToImage = os.path.join(self.path ,self.files[idx])
        image = Image.open(pathToImage)
        hrImage = self.hrTransform(image)
        lrImage = self.lrTransform(image)
        
        return {
            'hr': hrImage,
            'lr': lrImage
            }


path = "./img_align_celeba/img_align_celeba/" # path to Dataset

dataset = celebADataset(path)

trainSize = int(0.8 * len(dataset))
testSize = len(dataset) - trainSize
trainDataset, testDataset = torch.utils.data.random_split(dataset, [trainSize, testSize])

trainDataLoader, testDataLoader = (DataLoader(trainDataset, batch_size = batchSize,shuffle = True), 
                                   DataLoader(testDataset, batch_size = int(batchSize * 0.75), shuffle = True))

#@title
class FeatureExtractor(nn.Module):
    def __init__(self):
        super(FeatureExtractor, self).__init__()
        vgg19_model = vgg19(pretrained=True)
        self.feature_extractor = nn.Sequential(*list(vgg19_model.features.children())[:18])

    def forward(self, img):
        return self.feature_extractor(img)


class ResidualBlock(nn.Module):
    def __init__(self, in_channels = 64):
        super().__init__()    
        self.layers = nn.Sequential(
            nn.Conv2d(in_channels = in_channels,
                      out_channels = in_channels,
                      kernel_size = 3,
                      stride = 1,
                      padding = 1),
            nn.BatchNorm2d(in_channels, 0.8),
            nn.PReLU(),

            nn.Conv2d(in_channels = in_channels,
                      out_channels = in_channels,
                      kernel_size = 3,
                      stride = 1,
                      padding = 1),
            nn.BatchNorm2d(in_channels, 0.8))

    def forward(self, x):
        return self.layers(x) + x


class Generator(nn.Module):
    def __init__(self, in_channels = 3, out_channels = 3):
        super().__init__()

        self.conv1 = nn.Sequential(nn.Conv2d(in_channels = 3, 
                                             out_channels = 64, 
                                             kernel_size = 9, 
                                             stride = 1, 
                                             padding = 4),
                                   nn.PReLU()
        )

        residualBlocks = []

        for _ in range(10):
            residualBlocks.append(ResidualBlock(64))

        self.residualBlocks = nn.Sequential(*residualBlocks)
        
        self.conv2 = nn.Sequential(nn.Conv2d(in_channels = 64,
                                             out_channels = 64,
                                             kernel_size = 3,
                                             stride = 1,
                                             padding = 1,
                                             ),
                                   nn.BatchNorm2d(64, 0.8)
        )

        upsamplingLayers = []
        for out_features in range(2):
            upsamplingLayers += [
                # nn.Upsample(scale_factor=2),
                nn.Conv2d(64, 256, 3, 1, 1),
                nn.BatchNorm2d(256),
                nn.PixelShuffle(upscale_factor=2),
                nn.PReLU(),
            ]
        self.upsamplingLayers = nn.Sequential(*upsamplingLayers)

        # Final output layer
        self.conv3 = nn.Sequential(nn.Conv2d(64, out_channels, kernel_size=9, stride=1, padding=4), nn.Tanh())

    def forward(self, x):
        out1 = self.conv1(x)
        out = self.residualBlocks(out1)
        out2 = self.conv2(out)
        out = torch.add(out1, out2)
        out = self.upsamplingLayers(out)
        out = self.conv3(out)
        return out

class Discriminator(nn.Module):
    def __init__(self, inputShape):
        super().__init__()

        self.inputShape = inputShape
        in_channels, in_height, in_width = self.inputShape

        patch_h, patch_w = int(in_height / 2**4), int(in_width / 2**4)
        self.outShape = (1, patch_h, patch_w)

        layers = []

        def discriminatorBlock(in_channels, out_channels, firstBlock = False):
            layers = []
            
            layers += [
                nn.Conv2d(in_channels, out_channels, stride = 1, kernel_size = 3, padding = 1),
                nn.BatchNorm2d(out_channels),
                nn.LeakyReLU(0.2, inplace = True),
                nn.Conv2d(out_channels, out_channels, stride = 2, kernel_size = 3, padding = 1),
                nn.BatchNorm2d(out_channels),
                nn.LeakyReLU(0.2, inplace = True),
            ]

            if firstBlock:
                layers.pop(1)

            return layers
        
        for i, out_channels in enumerate([64, 128, 256, 512]):
            layers.extend(discriminatorBlock(in_channels, out_channels, firstBlock = (i == 0)))
            in_channels = out_channels

        layers.append(nn.Conv2d(in_channels = 512, out_channels = 1, kernel_size = (3,3), stride = 1, padding = 1))


        self.layers = nn.Sequential(*layers)

    def forward(self, x):
        return self.layers(x)

generator = Generator().to(device)
discriminator = Discriminator(inputShape = (channels, *hrShape)).to(device)
featureExtractor = FeatureExtractor()

featureExtractor.eval()

criterionGan = nn.MSELoss()
criterionDis = nn.L1Loss()

optG = optim.Adam(generator.parameters(), lr = lr, betas = (b1, b2))
optD = optim.Adam(discriminator.parameters(), lr = lr, betas = (b1, b2))

Tensor = torch.cuda.FloatTensor if device == 'cuda' else torch.Tensor

trainGenLosses, trainDisLosses, trainCounter = [], [], []
testGenLosses, testDisLosses = [], []

for epoch in range(2):
    genLoss, disLoss = 0, 0

    # Training
    for idx, imgs in tqdm(enumerate(trainDataLoader)):
        generator.train(), discriminator.train()
        imgsLr = imgs['lr'].to(device)
        imgsHr = imgs['hr'].to(device)

        valid = Variable(Tensor(np.ones((imgsLr.size(0), *discriminator.outShape))), requires_grad=False).to(device)
        fake = Variable(Tensor(np.zeros((imgsLr.size(0), *discriminator.outShape))), requires_grad=False).to(device)

        # Train Generator
        optG.zero_grad()
        
        genHr = generator(imgsLr)
        lossGan = criterionGan(discriminator(genHr), valid)
        genFeatures = featureExtractor(genHr)
        realFeatures = featureExtractor(imgsHr)

        lossContent = criterionDis(genFeatures, realFeatures.detach())

        lossG = lossContent + 1e-3 * lossGan
        lossG.backward()
        optG.step()

        # Train Discriminator
        optD.zero_grad()
        lossReal = criterionGan(discriminator(imgsHr), valid)
        lossFake = criterionGan(discriminator(genHr.detach()), fake)

        lossD = (lossReal + lossFake) / 2
        lossD.backward()
        optD.step()

        genLoss += lossG.item()
        trainGenLosses.append(lossG.item())
        disLoss += lossD.item()
        trainDisLosses.append(lossD.item())

        trainCounter.append()
        trainCounter.append(idx*batchSize + imgsLr.size(0) + epoch*len(trainDataLoader.dataset))

    # Testing
    genLoss, disLoss = 0, 0
    for idx, imgs in enumerate(testDataLoader):
        generator.eval(), discriminator.eval()

        imgsLr = imgs['lr']
        imgsHr = imgs['hr']

        valid = Variable(Tensor(np.ones((imgsLr.size(0), *discriminator.outShape))), requires_grad=False)
        fake = Variable(Tensor(np.zeros((imgsLr.size(0), *discriminator.outShape))), requires_grad=False)

        genHr = generator(imgsLr)
        lossGan = criterionGan(discriminator(genHr), valid)

        genFeatures = featureExtractor(genHr)
        realFeatures = featureExtractor(imgsHr)
        lossContent = criterionDis(genFeatures, realFeatures.detach())

        lossG = lossContent + 1e-3 * lossGan

        lossReal = criterionGan(discriminator(imgsHr), valid)
        lossFake = criterionGan(discriminator(genHr.detach()), fake)

        lossD = (lossReal + lossFake) / 2

        genLoss += lossG.item()
        disLoss += lossD.item()

        if random.uniform(0, 1) < 0.1:
            imgsLr = nn.functional.interpolate(imgsLr, scale_factor = 4)
            imgsHr = make_grid(imgsHr, nrow = 1, normalize = True)
            genHr = make_grid(genHr, nrow = 1, normalize = True)
            imgsLr = make_grid(imgsLr, nrow = 1, normalize = True)
            imgGrid = torch.cat((imgsHr, imgsLr, genHr), -1)
            save_image(imgGrid, f"images/{idx}.png", normalize = False)

    testGenLosses.append(genLoss / len(testDataLoader))
    testDisLosses.append(disLoss / len(testDataLoader))

    if np.argmin(testGenLosses) == len(testGenLosses)-1:
        torch.save(generator.state_dict(), "saved_models/generator.pth")
        torch.save(discriminator.state_dict(), "saved_models/discriminator.pth")

